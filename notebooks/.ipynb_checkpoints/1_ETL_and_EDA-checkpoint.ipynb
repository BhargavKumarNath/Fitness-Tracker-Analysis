{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91206ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80e76b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME set to: C:\\hadoop\n",
      "Checking for winutils.exe: True\n",
      "Checking for hadoop.dll: True\n",
      "SparkSession created successfully!\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "# CRITICAL: Add bin directory to PATH so hadoop.dll can be found\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop\\bin;\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "print(f\"HADOOP_HOME set to: {os.environ['HADOOP_HOME']}\")\n",
    "print(f\"Checking for winutils.exe: {os.path.exists(r'C:\\hadoop\\bin\\winutils.exe')}\")\n",
    "print(f\"Checking for hadoop.dll: {os.path.exists(r'C:\\hadoop\\bin\\hadoop.dll')}\")\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FitnessTrackerETL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress verbose logging\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"SparkSession created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4a3e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- steps: long (nullable = true)\n",
      " |-- calories_burned: double (nullable = true)\n",
      " |-- heart_rate_avg: long (nullable = true)\n",
      " |-- sleep_hours: double (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      "\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+\n",
      "|user_id|      date|steps|calories_burned|heart_rate_avg|sleep_hours|activity_type|\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+\n",
      "|      1|2023-06-26|10686|         950.16|           134|       5.28|      running|\n",
      "|      2|2023-06-26| 2062|          838.2|           127|       5.31|      cycling|\n",
      "|      3|2023-06-26| 1061|          826.0|           103|       9.94|  gym_workout|\n",
      "|      4|2023-06-26| 7028|         1105.4|           128|       4.08|       hiking|\n",
      "|      5|2023-06-26| 3980|          306.2|           107|        5.8|      walking|\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total records: 358497\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = \"../data_lake/raw/synthetic_user_data/\"\n",
    "# Read only Parquet files using pathGlobFilter\n",
    "df = spark.read \\\n",
    "    .option(\"pathGlobFilter\", \"*.parquet\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .parquet(raw_data_path)\n",
    "\n",
    "# Convert date string to actual date type\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"\\nTotal records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab02097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for null values in each column\n",
      "- Column 'user_id': 0 null values\n",
      "- Column 'date': 0 null values\n",
      "- Column 'steps': 0 null values\n",
      "- Column 'calories_burned': 0 null values\n",
      "- Column 'heart_rate_avg': 0 null values\n",
      "- Column 'sleep_hours': 0 null values\n",
      "- Column 'activity_type': 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "print(\"Checking for null values in each column\")\n",
    "for column in df.columns:\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    print(f\"- Column '{column}': {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274cf0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics for numerical columns:\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|            steps|  calories_burned|    heart_rate_avg|       sleep_hours|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|           358497|           358497|            358497|            358497|\n",
      "|   mean|6347.601795830928|754.0926114026058|119.52330145022134| 6.999896791326002|\n",
      "| stddev|6444.956548578772|441.3335340242217|26.986836543161438|1.7303321843744777|\n",
      "|    min|               50|            100.0|                60|               4.0|\n",
      "|    max|            24999|          2245.55|               169|              10.0|\n",
      "+-------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outlier Detection: Summary Statistics\n",
    "print(\"Descriptive statistics for numerical columns:\")\n",
    "df.describe(['steps', 'calories_burned', 'heart_rate_avg', 'sleep_hours']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d5c673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed DataFrame with new features:\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+-----------+-----------------------+\n",
      "|user_id|      date|steps|calories_burned|heart_rate_avg|sleep_hours|activity_type|day_of_week|calories_to_steps_ratio|\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+-----------+-----------------------+\n",
      "|      1|2023-06-26|10686|         950.16|           134|       5.28|      running|        Mon|    0.08891633913531724|\n",
      "|      2|2023-06-26| 2062|          838.2|           127|       5.31|      cycling|        Mon|     0.4064985451018429|\n",
      "|      3|2023-06-26| 1061|          826.0|           103|       9.94|  gym_workout|        Mon|     0.7785108388312912|\n",
      "|      4|2023-06-26| 7028|         1105.4|           128|       4.08|       hiking|        Mon|    0.15728514513375072|\n",
      "|      5|2023-06-26| 3980|          306.2|           107|        5.8|      walking|        Mon|    0.07693467336683417|\n",
      "|      6|2023-06-26|17215|        1451.75|           123|       7.48|       hiking|        Mon|    0.08433052570432763|\n",
      "|      7|2023-06-26|  128|          233.0|            67|       5.25|         yoga|        Mon|              1.8203125|\n",
      "|      8|2023-06-26|14774|         1613.7|           135|       8.08|       hiking|        Mon|    0.10922566671179099|\n",
      "|      9|2023-06-26| 1281|          480.1|           139|       8.96|      cycling|        Mon|    0.37478532396565184|\n",
      "|     10|2023-06-26|21766|         1658.3|           135|       5.29|       hiking|        Mon|    0.07618763208674079|\n",
      "+-------+----------+-----+---------------+--------------+-----------+-------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "from pyspark.sql.functions import dayofweek, date_format, when\n",
    "\n",
    "df_transformed = df.withColumn(\"day_of_week\", date_format(col(\"date\"), \"E\"))\n",
    "\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"calories_to_steps_ratio\",\n",
    "    when(col(\"steps\") > 0, col(\"calories_burned\") / col(\"steps\")).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Transformed DataFrame with new features:\")\n",
    "df_transformed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a951d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Summary Table\n",
      "+-------------+------------------+------------------+------------------+------------------+\n",
      "|activity_type|         avg_steps|      avg_calories|            avg_hr|         avg_sleep|\n",
      "+-------------+------------------+------------------+------------------+------------------+\n",
      "|       hiking|15990.771980223173| 1499.115077876141|134.47597271892283|6.9933325516405755|\n",
      "|  gym_workout|3496.2113607625715| 600.4481081606848|129.60854002528936| 7.015822585351613|\n",
      "|      cycling|2754.9361022986045| 775.4018412191749|124.51544405301777| 7.003108273808371|\n",
      "|         yoga|274.91280637973495|199.84642899026622| 79.47105273445135|  6.99331261483132|\n",
      "|      walking| 8981.155737222844| 558.7292030818091|  89.4894822482307| 6.998759238565741|\n",
      "|      running|12499.090853670528|1099.1238547914497|144.54433541129967| 6.998144775913941|\n",
      "|     swimming| 549.0619177018633| 549.4372476708074|134.44444875776398|6.9967175854037285|\n",
      "+-------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "from pyspark.sql.functions import avg, max, min\n",
    "\n",
    "activity_summary_df = df_transformed.groupBy(\"activity_type\").agg(\n",
    "    avg(\"steps\").alias(\"avg_steps\"),\n",
    "    avg(\"calories_burned\").alias(\"avg_calories\"),\n",
    "    avg(\"heart_rate_avg\").alias(\"avg_hr\"),\n",
    "    avg(\"sleep_hours\").alias(\"avg_sleep\")\n",
    ")\n",
    "print('Activity Summary Table')\n",
    "activity_summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849551eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved processed data to: ../data_lake/processed/fitness_data\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_to_load = df_transformed.withColumn(\"year\", year(col(\"date\")))\n",
    "df_to_load = df_to_load.withColumn(\"month\", month(col(\"date\")))\n",
    "\n",
    "processed_data_path = \"../data_lake/processed/fitness_data\"\n",
    "\n",
    "df_to_load.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(processed_data_path)\n",
    "\n",
    "print(f\"Successfully saved processed data to: {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dcbdf",
   "metadata": {},
   "source": [
    "# Phase 3 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ddbc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded processed data\n",
      "Inspecting the schema of our clean DataFrame:\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- steps: long (nullable = true)\n",
      " |-- calories_burned: double (nullable = true)\n",
      " |-- heart_rate_avg: long (nullable = true)\n",
      " |-- sleep_hours: double (nullable = true)\n",
      " |-- activity_type: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- calories_to_steps_ratio: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "\n",
      "Total records in processed data: 358497\n"
     ]
    }
   ],
   "source": [
    "processed_df = spark.read.parquet(\"../data_lake/processed/fitness_data\")\n",
    "\n",
    "print(\"Successfully loaded processed data\")\n",
    "print(\"Inspecting the schema of our clean DataFrame:\")\n",
    "processed_df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal records in processed data: {processed_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50cc3eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary view 'fitness_data_view' created\n",
      "\n",
      "Most active day of the week based on aveage steps:\n",
      "+-----------+------------------+\n",
      "|day_of_week|     average_steps|\n",
      "+-----------+------------------+\n",
      "|        Mon| 6386.372050104056|\n",
      "|        Sun| 6369.312855852672|\n",
      "|        Sat| 6352.275329438678|\n",
      "|        Thu| 6349.672144343661|\n",
      "|        Tue|6339.3561864373505|\n",
      "|        Wed| 6327.005222444733|\n",
      "|        Fri| 6309.039030902737|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running SQL Queries in Spark\n",
    "\n",
    "processed_df.createOrReplaceGlobalTempView(\"fitness_data_view\")\n",
    "print(\"Temporary view 'fitness_data_view' created\")\n",
    "\n",
    "most_active_day_query = \"\"\"\n",
    "    SELECT\n",
    "        day_of_week,\n",
    "        AVG(steps) as average_steps\n",
    "    FROM\n",
    "        global_temp.fitness_data_view\n",
    "    GROUP BY\n",
    "        day_of_week\n",
    "    ORDER BY\n",
    "        average_steps DESC\n",
    "\"\"\"\n",
    "\n",
    "most_active_day_query = spark.sql(most_active_day_query)\n",
    "\n",
    "print(\"\\nMost active day of the week based on aveage steps:\")\n",
    "most_active_day_query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8618c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "DenseMatrix([[ 1.00000000e+00,  8.44711501e-01,  2.69043946e-01,\n",
      "              -1.45788879e-03],\n",
      "             [ 8.44711501e-01,  1.00000000e+00,  5.19037015e-01,\n",
      "              -1.00738821e-03],\n",
      "             [ 2.69043946e-01,  5.19037015e-01,  1.00000000e+00,\n",
      "               8.07845917e-04],\n",
      "             [-1.45788879e-03, -1.00738821e-03,  8.07845917e-04,\n",
      "               1.00000000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Correlation Analysis\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# 1. Select numerical columns for correlation analysis\n",
    "numerical_cols = ['steps', 'calories_burned', 'heart_rate_avg', 'sleep_hours']\n",
    "corr_df = processed_df.select(numerical_cols)\n",
    "\n",
    "# 2. Assemble the columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "vector_df = assembler.transform(corr_df).select(\"features\")\n",
    "\n",
    "# 3. Calculate the Pearson correlation matrix\n",
    "matrix = Correlation.corr(vector_df, \"features\").head()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "corr_matrix = matrix[0]\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602eba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Correlation Matrix:\n",
      "                    steps  calories_burned  heart_rate_avg  sleep_hours\n",
      "steps            1.000000         0.844712        0.269044    -0.001458\n",
      "calories_burned  0.844712         1.000000        0.519037    -0.001007\n",
      "heart_rate_avg   0.269044         0.519037        1.000000     0.000808\n",
      "sleep_hours     -0.001458        -0.001007        0.000808     1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corr_matrix_pd = pd.DataFrame(corr_matrix.toArray(), columns=numerical_cols, index=numerical_cols)\n",
    "\n",
    "print(\"Formatted Correlation Matrix:\")\n",
    "print(corr_matrix_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff958874",
   "metadata": {},
   "source": [
    "# Phase 3: EDA User Segmentation with Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "835314c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User level summary DataFrame:\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|user_id|         avg_steps|     avg_calories|            avg_hr|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|     26|6896.6120218579235|788.0186338797813|120.73770491803279|\n",
      "|     29| 6570.404371584699|781.3319125683059|122.89617486338798|\n",
      "|    474| 5790.273224043716| 705.625737704918|118.13661202185793|\n",
      "|    964| 6567.163934426229|767.2180327868854| 120.2896174863388|\n",
      "|   1677| 5743.109289617486|700.7350273224044|118.37704918032787|\n",
      "|   1697| 6850.737704918033|800.8994535519124|119.39890710382514|\n",
      "|   1806| 6631.639344262295| 775.327267759563| 121.6775956284153|\n",
      "|   1950| 5834.808743169399|735.9052459016393|122.93989071038251|\n",
      "|     65| 5854.005464480874|704.1928961748634|116.60109289617486|\n",
      "|    191| 6885.551912568306| 766.481912568306|117.63387978142076|\n",
      "|    418| 6439.021857923497|762.5403825136611|119.20218579234972|\n",
      "|    541| 5362.284153005465|711.7765027322406|121.16939890710383|\n",
      "|    558| 5861.234972677596|726.6642622950819|117.74863387978142|\n",
      "|   1010| 6413.928961748634| 757.592349726776|120.44808743169399|\n",
      "|   1224| 6981.639344262295|786.1939890710381|119.39344262295081|\n",
      "|   1258|5570.3005464480875|725.2946994535517| 120.6120218579235|\n",
      "|   1277| 6402.595628415301|769.8322404371587|123.49180327868852|\n",
      "|   1360|5652.6994535519125|689.3680874316939|118.25136612021858|\n",
      "|   1840| 6528.814207650274|769.2534426229508|120.91256830601093|\n",
      "|    222| 6585.928961748634|775.5116393442623|120.22950819672131|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, stddev\n",
    "\n",
    "user_summary_df = processed_df.groupBy(\"user_id\").agg(\n",
    "    avg(\"steps\").alias(\"avg_steps\"),\n",
    "    avg(\"calories_burned\").alias(\"avg_calories\"),\n",
    "    avg(\"heart_rate_avg\").alias(\"avg_hr\")\n",
    ")\n",
    "\n",
    "print(\"User level summary DataFrame:\")\n",
    "user_summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7286d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users with their assigned cluster:\n",
      "+-------+------------------+-----------------+------------------+--------------------+--------------------+----------+\n",
      "|user_id|         avg_steps|     avg_calories|            avg_hr|   features_unscaled|            features|prediction|\n",
      "+-------+------------------+-----------------+------------------+--------------------+--------------------+----------+\n",
      "|     26|6896.6120218579235|788.0186338797813|120.73770491803279|[6896.61202185792...|[1.15062933929504...|         0|\n",
      "|     29| 6570.404371584699|781.3319125683059|122.89617486338798|[6570.40437158469...|[0.46695520115902...|         0|\n",
      "|    474| 5790.273224043716| 705.625737704918|118.13661202185793|[5790.27322404371...|[-1.1680631360302...|         1|\n",
      "|    964| 6567.163934426229|767.2180327868854| 120.2896174863388|[6567.16393442622...|[0.46016381205566...|         2|\n",
      "|   1677| 5743.109289617486|700.7350273224044|118.37704918032787|[5743.10928961748...|[-1.2669104873812...|         1|\n",
      "|   1697| 6850.737704918033|800.8994535519124|119.39890710382514|[6850.73770491803...|[1.05448480047080...|         0|\n",
      "|   1806| 6631.639344262295| 775.327267759563| 121.6775956284153|[6631.63934426229...|[0.59529298580041...|         0|\n",
      "|   1950| 5834.808743169399|735.9052459016393|122.93989071038251|[5834.80874316939...|[-1.0747244830918...|         2|\n",
      "|     65| 5854.005464480874|704.1928961748634|116.60109289617486|[5854.00546448087...|[-1.0344915152670...|         1|\n",
      "|    191| 6885.551912568306| 766.481912568306|117.63387978142076|[6885.55191256830...|[1.12744928609906...|         2|\n",
      "+-------+------------------+-----------------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Cluster sizes:\n",
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  523|\n",
      "|         2|  876|\n",
      "|         0|  560|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "features_for_clustering = ['avg_steps', 'avg_calories', 'avg_hr']\n",
    "assembler = VectorAssembler(inputCols=features_for_clustering, outputCol=\"features_unscaled\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features\", k=3, seed=1)\n",
    "pipeline = Pipeline(stages=[assembler, scaler, kmeans])\n",
    "\n",
    "model = pipeline.fit(user_summary_df)\n",
    "\n",
    "predictions = model.transform(user_summary_df)\n",
    "\n",
    "print(\"\\nUsers with their assigned cluster:\")\n",
    "predictions.show(10)\n",
    "\n",
    "print(\"\\nCluster sizes:\")\n",
    "predictions.groupBy('prediction').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5201d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Profiles\n",
      "+----------+-----------------+-----------------+------------------+---------+\n",
      "|prediction|        avg_steps|     avg_calories|            avg_hr|num_users|\n",
      "+----------+-----------------+-----------------+------------------+---------+\n",
      "|         1|5865.425111536008|716.2896343081629|117.72545946567205|      523|\n",
      "|         2|6297.943471317715|752.7675108541057| 119.7740100306909|      876|\n",
      "|         0|6875.600185402027|791.4707276541768|120.81017759562853|      560|\n",
      "+----------+-----------------+-----------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpret the Clusters\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "cluster_profiles = predictions.groupBy(\"prediction\").agg(\n",
    "    avg(\"avg_steps\").alias(\"avg_steps\"),\n",
    "    avg(\"avg_calories\").alias(\"avg_calories\"),\n",
    "    avg(\"avg_hr\").alias(\"avg_hr\"),\n",
    "    count(\"*\").alias(\"num_users\")\n",
    ").orderBy(\"avg_steps\")\n",
    "\n",
    "print(\"Cluster Profiles\")\n",
    "cluster_profiles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e300a",
   "metadata": {},
   "source": [
    "# Phase 4: Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d43857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Activity Types:\n",
      "+-------------+-----+\n",
      "|activity_type|count|\n",
      "+-------------+-----+\n",
      "|     swimming|51520|\n",
      "|  gym_workout|51405|\n",
      "|      cycling|51379|\n",
      "|       hiking|51171|\n",
      "|         yoga|51162|\n",
      "|      walking|51009|\n",
      "|      running|50851|\n",
      "+-------------+-----+\n",
      "\n",
      "Training dataset count: 287006\n",
      "Testing dataset count: 71491\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_ml = spark.read.parquet(\"../data_lake/processed/fitness_data\")\n",
    "\n",
    "# Task 1: Predict Activity Type \n",
    "features = ['steps', 'calories_burned', 'heart_rate_avg', 'sleep_hours']\n",
    "target = 'activity_type'\n",
    "\n",
    "model_df = df_ml.select(features + [target])\n",
    "print(\"Distribution of Activity Types:\")\n",
    "model_df.groupBy(target).count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "train_df, test_df = model_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training dataset count: {train_df.count()}\")\n",
    "print(f\"Testing dataset count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6600f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classification model...\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"activity_type\", outputCol=\"label\")\n",
    "\n",
    "feature_assembler = VectorAssembler(inputCols=features, outputCol=\"features_unscaled\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[label_indexer, feature_assembler, scaler, lr])\n",
    "\n",
    "print(\"Training the classification model...\")\n",
    "model = pipeline.fit(train_df)\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59672ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.8398\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. Generate predictions on the test data using the trained classification model\n",
    "classification_predictions = model.transform(test_df)\n",
    "\n",
    "# 2. Initialize the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# 3. Evaluate the new classification_predictions DataFrame\n",
    "accuracy = evaluator.evaluate(classification_predictions)\n",
    "\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca3b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+--------------------+\n",
      "|activity_type|label|prediction|         probability|\n",
      "+-------------+-----+----------+--------------------+\n",
      "|         yoga|  3.0|       3.0|[3.87017488146527...|\n",
      "|         yoga|  3.0|       3.0|[3.04268689500068...|\n",
      "|         yoga|  3.0|       3.0|[2.94673553441807...|\n",
      "|         yoga|  3.0|       3.0|[0.02647790848949...|\n",
      "|         yoga|  3.0|       3.0|[1.24677506315505...|\n",
      "|         yoga|  3.0|       3.0|[1.29283148452384...|\n",
      "|         yoga|  3.0|       3.0|[1.24583886120191...|\n",
      "|         yoga|  3.0|       3.0|[1.00279981425510...|\n",
      "|         yoga|  3.0|       3.0|[3.36103520880797...|\n",
      "|         yoga|  3.0|       3.0|[2.77254885583225...|\n",
      "+-------------+-----+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"activity_type\", \"label\", \"prediction\", \"probability\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f877d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+\n",
      "|activity_type|prediction|         probability|\n",
      "+-------------+----------+--------------------+\n",
      "|     swimming|       0.0|[0.99943249786062...|\n",
      "|     swimming|       0.0|[0.99940005791853...|\n",
      "|     swimming|       0.0|[0.99937997964430...|\n",
      "|     swimming|       0.0|[0.99936755053689...|\n",
      "|     swimming|       0.0|[0.99936602848671...|\n",
      "|     swimming|       0.0|[0.99935718772116...|\n",
      "|     swimming|       0.0|[0.99935592779982...|\n",
      "|     swimming|       0.0|[0.99935098457808...|\n",
      "|     swimming|       0.0|[0.99934422961770...|\n",
      "|     swimming|       0.0|[0.99934220189740...|\n",
      "+-------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0| 1125|\n",
      "|  6.0|       1.0|  586|\n",
      "|  4.0|       6.0| 2242|\n",
      "|  5.0|       1.0|  204|\n",
      "|  1.0|       1.0| 6849|\n",
      "|  1.0|       6.0|  183|\n",
      "|  0.0|       1.0|   14|\n",
      "|  6.0|       4.0| 1776|\n",
      "|  2.0|       2.0| 7335|\n",
      "|  1.0|       0.0|  114|\n",
      "|  6.0|       6.0| 7759|\n",
      "|  4.0|       4.0| 8021|\n",
      "|  1.0|       5.0|  276|\n",
      "|  2.0|       1.0| 1825|\n",
      "|  1.0|       2.0| 2849|\n",
      "|  5.0|       5.0| 9947|\n",
      "|  0.0|       0.0|10051|\n",
      "|  1.0|       3.0|    8|\n",
      "|  0.0|       2.0|  245|\n",
      "|  3.0|       3.0|10073|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Top 10 predictions with probability\n",
    "predictions.select(\"activity_type\", \"prediction\", \"probability\").orderBy(col(\"probability\").desc()).show(10)\n",
    "\n",
    "# Optional: confusion matrix\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f02551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for regression task\n",
      "Training set size: 287006\n",
      "Testing set size: 71491\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Predict Calories Burned\n",
    "\n",
    "features_reg = ['steps', 'heart_rate_avg', 'sleep_hours', 'activity_type']\n",
    "target_reg = \"calories_burned\"\n",
    "\n",
    "model_reg_df = df_ml.select(features_reg + [target_reg])\n",
    "\n",
    "train_reg_df, test_reg_df = model_reg_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(\"Data prepared for regression task\")\n",
    "print(f\"Training set size: {train_reg_df.count()}\")\n",
    "print(f\"Testing set size: {test_reg_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd363e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "categorical_col = \"activity_type\"\n",
    "numerical_cols = ['steps', 'heart_rate_avg', 'sleep_hours']\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=categorical_col, outputCol=\"activity_idx\")\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCol=\"activity_idx\", outputCol=\"activity_vec\")\n",
    "\n",
    "assembler_inputs = numerical_cols + [\"activity_vec\"]\n",
    "feature_assembler_reg = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "lr_reg = LinearRegression(featuresCol=\"features\", labelCol=\"calories_burned\")\n",
    "\n",
    "pipeline_reg = Pipeline(stages=[string_indexer, one_hot_encoder, feature_assembler_reg, lr_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d179175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the regression model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the regression model...\")\n",
    "model_reg = pipeline_reg.fit(train_reg_df)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e77dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on the test set:\n",
      "+---------------+------------------+\n",
      "|calories_burned|        prediction|\n",
      "+---------------+------------------+\n",
      "|          242.0|188.13391660088507|\n",
      "|          282.0| 188.5605137106498|\n",
      "|          234.0|188.75902697505865|\n",
      "|          125.0| 188.9788949958397|\n",
      "|          163.0|188.35986152772023|\n",
      "|          286.0|189.02363682785875|\n",
      "|          140.0|189.38861525331225|\n",
      "|          292.0|188.51746345919435|\n",
      "|          199.0|188.32785208897747|\n",
      "|          167.0|188.30982782537615|\n",
      "+---------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "predictions_reg = model_reg.transform(test_reg_df)\n",
    "\n",
    "print(\"predictions on the test set:\")\n",
    "predictions_reg.select(\"calories_burned\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccdf374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 131.2747\n"
     ]
    }
   ],
   "source": [
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=\"calories_burned\", predictionCol=\"prediction\", metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator_rmse.evaluate(predictions_reg)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e73b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data: 0.9114\n"
     ]
    }
   ],
   "source": [
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=\"calories_burned\", predictionCol=\"prediction\",metricName=\"r2\"\n",
    ")\n",
    "r2 = evaluator_r2.evaluate(predictions_reg)\n",
    "print(f\"R-squared (R2) on test data: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a39759",
   "metadata": {},
   "source": [
    "# Phase 5: Advanced Analytics - Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query started!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "stream_schema = StructType([\n",
    "    StructField(\"user_id\", LongType()),\n",
    "    StructField(\"timestamp\", StringType()), \n",
    "    StructField(\"steps\", LongType()),\n",
    "    StructField(\"calories_burned\", DoubleType()),\n",
    "    StructField(\"heart_rate_avg\", LongType()),\n",
    "    StructField(\"sleep_hours\", DoubleType()),\n",
    "    StructField(\"activity_type\", StringType())\n",
    "])\n",
    "\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(stream_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .parquet(\"C:/Project/Fitness Tracker Analysis/data_lake/streaming_input/\")\n",
    "\n",
    "processed_stream_df = streaming_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "query = processed_stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"streaming_health_data\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cd99210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•”==============================================================================â•—\n",
      "â•‘  Auto-Refresh #11 | Records: 43 | 15:43:52                                   â•‘\n",
      "â•š==============================================================================â•\n",
      "\n",
      "LATEST 30 RECORDS:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "+-------+-------------------+-----+---------------+--------------+-----------+-------------+\n",
      "|user_id|timestamp          |steps|calories_burned|heart_rate_avg|sleep_hours|activity_type|\n",
      "+-------+-------------------+-----+---------------+--------------+-----------+-------------+\n",
      "|164    |2025-10-04 15:43:40|386  |30.88          |135           |7.88       |running      |\n",
      "|1998   |2025-10-04 15:43:40|130  |9.1            |122           |6.64       |hiking       |\n",
      "|1484   |2025-10-04 15:43:40|15   |448.0          |111           |6.21       |swimming     |\n",
      "|1306   |2025-10-04 15:43:40|287  |20.09          |120           |6.8        |hiking       |\n",
      "|130    |2025-10-04 15:43:40|30   |244.0          |86            |6.38       |yoga         |\n",
      "|1940   |2025-10-04 15:43:40|148  |8.88           |134           |8.47       |cycling      |\n",
      "|993    |2025-10-04 15:43:40|103  |690.0          |129           |8.67       |gym_workout  |\n",
      "|1455   |2025-10-04 15:43:40|184  |11.04          |126           |6.76       |cycling      |\n",
      "|1684   |2025-10-04 15:43:40|122  |8.54           |148           |8.16       |hiking       |\n",
      "|1473   |2025-10-04 15:43:30|24   |123.0          |93            |7.81       |yoga         |\n",
      "|1353   |2025-10-04 15:43:30|211  |14.77          |140           |7.2        |hiking       |\n",
      "|1497   |2025-10-04 15:43:30|38   |150.0          |97            |7.45       |yoga         |\n",
      "|1912   |2025-10-04 15:43:30|153  |6.12           |92            |8.14       |walking      |\n",
      "|1140   |2025-10-04 15:43:30|157  |10.99          |148           |6.71       |hiking       |\n",
      "|1510   |2025-10-04 15:43:20|280  |22.4           |157           |7.62       |running      |\n",
      "|1939   |2025-10-04 15:43:20|102  |519.0          |126           |7.73       |gym_workout  |\n",
      "|144    |2025-10-04 15:43:20|223  |13.38          |107           |8.28       |cycling      |\n",
      "|1154   |2025-10-04 15:43:20|93   |3.72           |107           |7.19       |walking      |\n",
      "|625    |2025-10-04 15:43:20|100  |7.0            |132           |8.22       |hiking       |\n",
      "|1511   |2025-10-04 15:43:20|374  |26.18          |137           |7.54       |hiking       |\n",
      "|1914   |2025-10-04 15:43:20|135  |647.0          |157           |7.35       |gym_workout  |\n",
      "|1311   |2025-10-04 15:43:10|108  |709.0          |144           |7.04       |gym_workout  |\n",
      "|499    |2025-10-04 15:43:10|137  |8.22           |129           |7.35       |cycling      |\n",
      "|1259   |2025-10-04 15:43:10|39   |462.0          |124           |7.36       |swimming     |\n",
      "|841    |2025-10-04 15:43:10|144  |10.08          |149           |8.9        |hiking       |\n",
      "|715    |2025-10-04 15:43:10|426  |34.08          |122           |7.61       |running      |\n",
      "|1471   |2025-10-04 15:43:10|375  |26.25          |138           |6.91       |hiking       |\n",
      "|1743   |2025-10-04 15:42:41|45   |367.0          |123           |6.74       |swimming     |\n",
      "|132    |2025-10-04 15:42:41|49   |260.0          |68            |7.5        |yoga         |\n",
      "|1313   |2025-10-04 15:42:41|131  |795.0          |125           |6.43       |gym_workout  |\n",
      "+-------+-------------------+-----+---------------+--------------+-----------+-------------+\n",
      "\n",
      "\n",
      "ACTIVITY BREAKDOWN:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "+-------------+-----+---------+------------+\n",
      "|activity_type|count|avg_steps|avg_calories|\n",
      "+-------------+-----+---------+------------+\n",
      "|hiking       |11   |219.45   |15.36       |\n",
      "|gym_workout  |9    |110.67   |587.56      |\n",
      "|yoga         |6    |27.17    |192.17      |\n",
      "|running      |5    |360.6    |28.85       |\n",
      "|swimming     |5    |36.6     |394.2       |\n",
      "|cycling      |5    |191.4    |11.48       |\n",
      "|walking      |2    |123.0    |4.92        |\n",
      "+-------------+-----+---------+------------+\n",
      "\n",
      "Streaming Query: ACTIVE\n",
      "   Last batch processed: 5 records\n",
      "\n",
      "ðŸ’¡ Press 'Interrupt Kernel' (â– ) to stop auto-refresh\n",
      "\n",
      "\n",
      "Auto-refresh stopped by user\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Starting auto-refresh display...\")\n",
    "print(\"â¸Press 'Interrupt Kernel' (â–  button) to stop\\n\")\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    refresh_count = 0\n",
    "    while True:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Try to get data from memory table\n",
    "        try:\n",
    "            df = spark.sql(\"SELECT * FROM streaming_health_data ORDER BY timestamp DESC LIMIT 30\")\n",
    "            total_count = spark.sql(\"SELECT COUNT(*) as count FROM streaming_health_data\").first()[\"count\"]\n",
    "\n",
    "            \n",
    "            # Display header\n",
    "            print(\"â•”\" + \"=\"*78 + \"â•—\")\n",
    "            print(f\"â•‘  Auto-Refresh #{refresh_count} | Records: {total_count} | {time.strftime('%H:%M:%S')}\".ljust(79) + \"â•‘\")\n",
    "            print(\"â•š\" + \"=\"*78 + \"â•\")\n",
    "            \n",
    "            # Show data\n",
    "            if total_count > 0:\n",
    "                print(\"\\nLATEST 30 RECORDS:\")\n",
    "                print(\"â”€\"*80)\n",
    "                df.show(30, truncate=False)\n",
    "                \n",
    "                # Activity breakdown\n",
    "                print(\"\\nACTIVITY BREAKDOWN:\")\n",
    "                print(\"â”€\"*80)\n",
    "                spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        activity_type, \n",
    "                        COUNT(*) as count,\n",
    "                        ROUND(AVG(steps), 2) as avg_steps,\n",
    "                        ROUND(AVG(calories_burned), 2) as avg_calories\n",
    "                    FROM streaming_health_data \n",
    "                    GROUP BY activity_type \n",
    "                    ORDER BY count DESC\n",
    "                \"\"\").show(truncate=False)\n",
    "            else:\n",
    "                print(\"\\nWaiting for data... (files being processed)\")\n",
    "            \n",
    "            # Check query status\n",
    "            try:\n",
    "                if 'query' in globals():\n",
    "                    if query.isActive:\n",
    "                        print(f\"Streaming Query: ACTIVE\")\n",
    "                        \n",
    "                        # Show last progress if available\n",
    "                        if query.lastProgress:\n",
    "                            print(f\"   Last batch processed: {query.lastProgress.get('batchId', 'N/A')} records\")\n",
    "                    else:\n",
    "                        print(f\"âœ— Streaming Query: STOPPED\")\n",
    "                        print(\"\\nQuery stopped - breaking refresh loop\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"Query variable not found\")\n",
    "            except:\n",
    "                print(f\"Cannot check query status\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Waiting for data... ({str(e)[:50]})\")\n",
    "        \n",
    "        print(\"\\nðŸ’¡ Press 'Interrupt Kernel' (â– ) to stop auto-refresh\")\n",
    "        \n",
    "        refresh_count += 1\n",
    "        time.sleep(3)  # Refresh every 3 seconds\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nAuto-refresh stopped by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcffe75",
   "metadata": {},
   "source": [
    "# Phase 5: Streaming Aggregations with Watermarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfdc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession and necessary libraries are ready for Phase 5.\n"
     ]
    }
   ],
   "source": [
    "# Phase 5 Setup Cell: Imports and SparkSession Initialization\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- ADD THESE LINES ---\n",
    "# Point Spark to the correct Python executable for both driver and workers\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "# ---------------------\n",
    "\n",
    "# --- SparkSession Initialization ---\n",
    "# This setup is necessary for running Spark locally on Windows.\n",
    "# Adjust the path if your Hadoop installation is in a different location.\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop\\bin;\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Create or get the existing SparkSession\n",
    "# Create SparkSession with more memory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FitnessTrackerRecommendations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress verbose logging for a cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"SparkSession and necessary libraries are ready for Phase 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the incoming streaming data\n",
    "stream_schema = StructType([\n",
    "    StructField(\"user_id\", LongType()),\n",
    "    StructField(\"timestamp\", StringType()), \n",
    "    StructField(\"steps\", LongType()),\n",
    "    StructField(\"calories_burned\", DoubleType()),\n",
    "    StructField(\"heart_rate_avg\", LongType()),\n",
    "    StructField(\"sleep_hours\", DoubleType()),\n",
    "    StructField(\"activity_type\", StringType())\n",
    "])\n",
    "\n",
    "# Read the streaming data from the source directory\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(stream_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .parquet(\"C:/Project/Fitness Tracker Analysis/data_lake/streaming_input/\") # Make sure this path is correct\n",
    "\n",
    "# Convert the timestamp string to a proper timestamp type\n",
    "processed_stream_df = streaming_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Start the streaming query and write to an in-memory table\n",
    "query = processed_stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"streaming_health_data\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "259d79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'query' in globals() and query.isActive:\n",
    "        print(\"Stopping the previous memory-based query...\")\n",
    "        query.stop()\n",
    "        print(\"Previous query stopped.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not stop query (it may already be stopped): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8811008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete and ready.\n"
     ]
    }
   ],
   "source": [
    "# Advanced Stream: Tumbling Window Aggregation\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Adjust the path if your Hadoop installation is in a different location.\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop\\bin;\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# --- SparkSession Initialization ---\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop\\bin;\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Create or get the existing SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FitnessTrackerRecommendations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# --- Streaming DataFrame Setup ---\n",
    "stream_schema = StructType([\n",
    "    StructField(\"user_id\", LongType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"steps\", LongType()),\n",
    "    StructField(\"calories_burned\", DoubleType()),\n",
    "    StructField(\"heart_rate_avg\", LongType()),\n",
    "    StructField(\"sleep_hours\", DoubleType()),\n",
    "    StructField(\"activity_type\", StringType())\n",
    "])\n",
    "\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(stream_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .parquet(\"C:/Project/Fitness Tracker Analysis/data_lake/streaming_input/\")\n",
    "\n",
    "processed_stream_df = streaming_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "print(\"Setup complete and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03431119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the windowed aggregation\n",
    "windowed_counts_df = processed_stream_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"30 seconds\"),\n",
    "        \"activity_type\"\n",
    "    ).count()\n",
    "\n",
    "# Write to an in-memory table named \"windowed_results\"\n",
    "query_memory = windowed_counts_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"windowed_results\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started. Now wait for 2 minutes before running the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82fb871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------------+-----+\n",
      "|window                                    |activity_type|count|\n",
      "+------------------------------------------+-------------+-----+\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|yoga         |1    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|running      |1    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|swimming     |1    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|hiking       |1    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|cycling      |2    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|yoga         |3    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|running      |2    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|hiking       |2    |\n",
      "|{2025-10-04 18:02:00, 2025-10-04 18:02:30}|walking      |1    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|gym_workout  |3    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|cycling      |1    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|swimming     |1    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|running      |2    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|hiking       |2    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|swimming     |2    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|walking      |1    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|hiking       |5    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|yoga         |1    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|walking      |2    |\n",
      "|{2025-10-04 18:01:30, 2025-10-04 18:02:00}|cycling      |2    |\n",
      "+------------------------------------------+-------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to see the latest results from the stream\n",
    "spark.sql(\"SELECT * FROM windowed_results ORDER BY window.start DESC\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26c0eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings data prepared for ALS:\n",
      "+-------+-------------+------+-------+\n",
      "|user_id|activity_type|rating|item_id|\n",
      "+-------+-------------+------+-------+\n",
      "|     41|      walking|    32|    5.0|\n",
      "|     52|         yoga|    35|    6.0|\n",
      "|    110|       hiking|    26|    2.0|\n",
      "|    132|      cycling|    26|    0.0|\n",
      "|    187|         yoga|    19|    6.0|\n",
      "|    347|  gym_workout|    40|    1.0|\n",
      "|    348|      running|    35|    3.0|\n",
      "|    395|      cycling|    23|    0.0|\n",
      "|    398|     swimming|    26|    4.0|\n",
      "|    658|         yoga|    23|    6.0|\n",
      "+-------+-------------+------+-------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Advanced Analytics\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als_df = spark.read.parquet(\"../data_lake/processed/fitness_data\")\n",
    "\n",
    "ratings_df = als_df.groupBy(\"user_id\", \"activity_type\").count().withColumnRenamed(\"count\", \"rating\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"activity_type\", outputCol=\"item_id\")\n",
    "indexer_model = indexer.fit(ratings_df)\n",
    "ratings_indexed_df = indexer_model.transform(ratings_df)\n",
    "\n",
    "print(\"Ratings data prepared for ALS:\")\n",
    "ratings_indexed_df.show(10)\n",
    "\n",
    "(training, test) = ratings_indexed_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e727d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the ALS recommendation model...\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Training the ALS Model and Making Recommendations\n",
    "\n",
    "als = ALS(\n",
    "    maxIter=5,\n",
    "    regParam=0.01,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "print(\"Training the ALS recommendation model...\")\n",
    "model_als = als.fit(training)\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd633128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on the test set\n",
      "+-------+-------------+------+-------+----------+\n",
      "|user_id|activity_type|rating|item_id|prediction|\n",
      "+-------+-------------+------+-------+----------+\n",
      "|    148|      walking|    23|      5| 19.479492|\n",
      "|    471|      cycling|    17|      0| 26.591143|\n",
      "|   1342|     swimming|    37|      4| 12.510277|\n",
      "|   1342|      walking|    15|      5| 10.966165|\n",
      "|   1580|      cycling|    21|      0| 15.510708|\n",
      "|   1645|  gym_workout|    22|      1| 18.292093|\n",
      "|   1645|      walking|    26|      5| 25.087502|\n",
      "|   1829|      running|    27|      3| 13.023116|\n",
      "|   1829|     swimming|    29|      4|   9.59326|\n",
      "|   1959|     swimming|    29|      4| 21.220932|\n",
      "+-------+-------------+------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Root Mean Square Error for ALS model: 9.1810\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "predictions_als = model_als.transform(test)\n",
    "print(\"Predictions on the test set\")\n",
    "predictions_als.show(10)\n",
    "\n",
    "evaluator_als = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse_als = evaluator_als.evaluate(predictions_als)\n",
    "print(f\"\\nRoot Mean Square Error for ALS model: {rmse_als:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01288e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the final lookup table to get activity name\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# --- Stop any previous Spark session ---\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        print(\"Stopping existing Spark session...\")\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf3b232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new SparkSession...\n",
      "SparkSession ready.\n",
      "\n",
      "Preparing data for ALS model...\n",
      "âœ… Data prepared.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PATH\"] = r\"C:\\hadoop\\bin;\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# --- Create SparkSession with MINIMAL resources ---\n",
    "print(\"Starting a new SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FitnessTrackerALS_Minimal\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.default.parallelism\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"SparkSession ready.\")\n",
    "\n",
    "# --- 1. Load and Prepare Data ---\n",
    "print(\"\\nPreparing data for ALS model...\")\n",
    "als_df = spark.read.parquet(\"../data_lake/processed/fitness_data\")\n",
    "ratings_df = als_df.groupBy(\"user_id\", \"activity_type\").count().withColumnRenamed(\"count\", \"rating\")\n",
    "\n",
    "# Index activities\n",
    "indexer = StringIndexer(inputCol=\"activity_type\", outputCol=\"item_id\")\n",
    "indexer_model = indexer.fit(ratings_df)\n",
    "ratings_indexed_df = indexer_model.transform(ratings_df)\n",
    "\n",
    "# Split data\n",
    "(training, test) = ratings_indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"âœ… Data prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19253996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the ALS recommendation model...\n",
      "Training complete.\n",
      "\n",
      "Generating top 3 activity recommendations for all users...\n",
      "Recommendations generated.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Train the ALS Model ---\n",
    "print(\"\\nTraining the ALS recommendation model...\")\n",
    "als = ALS(\n",
    "    maxIter=5, \n",
    "    regParam=0.01, \n",
    "    userCol=\"user_id\", \n",
    "    itemCol=\"item_id\", \n",
    "    ratingCol=\"rating\", \n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "model_als = als.fit(training)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 3. Generate Recommendations ---\n",
    "print(\"\\nGenerating top 3 activity recommendations for all users...\")\n",
    "user_recs = model_als.recommendForAllUsers(3)\n",
    "print(\"Recommendations generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1f2c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting final recommendations...\n",
      "Converting to Pandas for safe processing...\n",
      "Formatting complete.\n",
      "\n",
      "Final, human-readable recommendations:\n",
      "Total rows: 5877\n",
      "\n",
      "First 15 recommendations:\n",
      "User 1: yoga (score: 40.40)\n",
      "User 1: gym_workout (score: 31.03)\n",
      "User 1: running (score: 28.01)\n",
      "User 2: yoga (score: 56.91)\n",
      "User 2: cycling (score: 30.82)\n",
      "User 2: gym_workout (score: 30.66)\n",
      "User 3: hiking (score: 30.57)\n",
      "User 3: running (score: 28.17)\n",
      "User 3: cycling (score: 27.86)\n",
      "User 4: cycling (score: 29.64)\n",
      "User 4: yoga (score: 29.20)\n",
      "User 4: swimming (score: 28.77)\n",
      "User 5: gym_workout (score: 35.39)\n",
      "User 5: swimming (score: 28.71)\n",
      "User 5: cycling (score: 27.60)\n",
      "\n",
      "Task complete. Stopping Spark session.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. ALTERNATIVE APPROACH: Convert to Pandas instead of Spark operations ---\n",
    "print(\"\\nFormatting final recommendations...\")\n",
    "\n",
    "# Get the activity labels mapping\n",
    "id_to_activity_map = indexer_model.labels\n",
    "\n",
    "# Convert recommendations to Pandas (this avoids the problematic join)\n",
    "print(\"Converting to Pandas for safe processing...\")\n",
    "user_recs_pd = user_recs.toPandas()\n",
    "\n",
    "# Process in Pandas\n",
    "results = []\n",
    "for _, row in user_recs_pd.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    for rec in row['recommendations']:\n",
    "        item_id = int(rec['item_id'])\n",
    "        rating = float(rec['rating'])\n",
    "        activity = id_to_activity_map[item_id]\n",
    "        results.append({\n",
    "            'user_id': user_id,\n",
    "            'activity_type_rec': activity,\n",
    "            'predicted_rating': rating\n",
    "        })\n",
    "\n",
    "# Display results\n",
    "print(\"Formatting complete.\\n\")\n",
    "print(\"Final, human-readable recommendations:\")\n",
    "print(f\"Total rows: {len(results)}\")\n",
    "print(\"\\nFirst 15 recommendations:\")\n",
    "for i, rec in enumerate(results[:15]):\n",
    "    print(f\"User {rec['user_id']}: {rec['activity_type_rec']} (score: {rec['predicted_rating']:.2f})\")\n",
    "\n",
    "# --- Clean up ---\n",
    "print(\"\\nTask complete. Stopping Spark session.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58bb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
